---
title: "Foresasting Wind Speed at Dogger Bank"
author: "Mehrnaz Kashfi"
date: '`r Sys.Date()`'
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(stringr)
library(lubridate)
library(dplyr)
library(zoo)
library(corrplot)
library(forecast)
library(ggplot2)
library(tseries)
library(reshape2)
library(lmtest)
library(car)
library(olsrr)
library(dplyr)
library(randomForest)
library(rpart)
library(e1071)  # For SVR
library(caret)  # For model evaluation and metrics
library(caTools) # For sample.split ML
# library(tidyr)
# library(ggplot2)
```
# Data Preprocessing

Data preprocessing involves preparing the dataset for analysis which includes loading the data, filtering based on specific conditions, and ensuring data types are appropriate for analysis. This section is detailed to ensuring the dataset is ready for in-depth analysis without repeating the specific steps here.

#### Loading Data & Geographic Filtering & Removing LAT, LONG
```{r load-data}
# Read the dataset and skip first row
data <- read.csv("C:/Users/User/Documents/WRFdata_May2018.csv", header = T, skip = 1)

dim(data)

# Filter location
data <- data[!is.na(data$XLAT) & !is.na(data$XLONG), ]

# Convert the first two columns to numeric type if they are not already
data$XLAT <- as.numeric(as.character(data$XLAT))
data$XLONG <- as.numeric(as.character(data$XLONG))

# Now, ensure that the latitude and longitude values are within the appropriate range
# Latitude ranges from -90 to 90 and Longitude ranges from -180 to 180
data <- data[data$XLAT >= -90 & data$XLAT <= 90 & data$XLONG >= -180 & data$XLONG <= 180, ]

# Filter the data based on geographic coordinates
data <- data[data$XLAT == 54.51 & data$XLONG == 1.947, ]
dim(data)

#Assuming that latitude and longitude are not required for ongoing analysis..
data <- data[, -c(1, 2)]

dim(data)
```

#### Reshaping data
```{r reshape-data}
# Define column names
col_names <- c("TSK", "PSFC", "U10", "V10", "Q2", "RAINC", "RAINNC", "SNOW", "TSLB", "SMOIS")

# Define column indexes for each row
cols <- seq(1, ncol(data), by=10)
length(cols) #2480/10 = 248

#Generates an empty vector
emp_df <- c()

#Reread the dataset, without 2 first columns
data2 <- read.csv("C:/Users/User/Documents/WRFdata_May2018.csv", header = T) %>% dplyr::select(-c(1,2))


#Reshap the dataset
for (i in seq_along(cols)) { 
  df0 <- data %>% dplyr::select(cols[i]:(cols[i]+9)) #Selects a subset of data from 'data' that start from cols[i]
  
  dt <- colnames(data2)[cols[i]] #Extract the column name of dates from data2
  
  df0_ll <- cbind.data.frame(dt, df0) #Combining the selected data with the date name
  
  colnames(df0_ll) <- c("date", col_names) #Setting column name of date for df0_ll
  
  emp_df <- rbind.data.frame(emp_df, df0_ll) #Appending the new row to the existing dataframe

}


#Correct the value of date_time columns
date_time <- str_remove(emp_df$date, "X")

date_time <- gsub(".", "-", date_time, fixed = TRUE)

emp_df$date <- sub("X\\.2225", "X31.05.2018.21.00", emp_df$date)


#Remove the column of date to date_time
new_df <- emp_df %>%
  dplyr::select(-date) %>%  
  mutate(date_time=date_time) %>%
  relocate(date_time, .before = TSK)


dim(new_df)
head(new_df)
```

#### Calculating wind speed
```{r wind-speed}
#Step1: Conversion to Numeric Data Type
new_df$U10 <- as.numeric(as.character(new_df$U10))
new_df$V10 <- as.numeric(as.character(new_df$V10))
na_count_u10 <- sum(is.na(new_df$U10))
na_count_v10 <- sum(is.na(new_df$V10))
print(paste("Number of NA values in U10: ", na_count_u10))
print(paste("Number of NA values in V10: ", na_count_v10))

#Step2: Handling Missing Values
replace_na_with_neighbors <- function(x) {
  # Loop through each element in the vector
  for (i in 1:length(x)) {
    # Check if the current element is NA
    if (is.na(x[i])) {
      # Find non-NA values before and after the current NA
      before <- x[1:i][!is.na(x[1:i])]
      after <- x[i:length(x)][!is.na(x[i:length(x)])]
      
      # Use the last value from 'before' and the first from 'after'
      if (length(before) > 0 && length(after) > 0) {
        x[i] <- mean(c(tail(before, 1), head(after, 1)), na.rm = TRUE)
      } else if (length(before) > 0) {
        x[i] <- tail(before, 1)
      } else if (length(after) > 0) {
        x[i] <- head(after, 1)
      }
    }
  }
  return(x)
}
new_df$U10 <- replace_na_with_neighbors(new_df$U10)
new_df$V10 <- replace_na_with_neighbors(new_df$V10)
sum(is.na(new_df$U10))  # Should be 0 if all NAs were replaced
sum(is.na(new_df$V10))  # Should be 0 if all NAs were replaced


#Step3: Wind Speed Calculation
new_df$WindSpeed <- sqrt(new_df$U10^2 + new_df$V10^2)

sum(is.infinite(new_df$WindSpeed))  # Inf or -Inf
sum(is.na(new_df$WindSpeed))        # NA

#After calculate wind speed, delete U10 and V10
glimpse(new_df)
new_df <- subset(new_df, select = -c(V10, U10))
```

#### Preprocessing for another columns
In this part, first check NA values and handle them, then detect and delete some variable that don't have enough variation to check correlation test and another steps.

For handling missing data in secondary variables, linear interpolation was used. This method assumes that changes between available data points are gradual and linear, helping to estimate missing values effectively. This choice was made due to its simplicity and efficiency in maintaining the overall trend of the data without adding unnecessary complexity to the analysis.

```{r preprocessing_remain_column}

# Check the structure of the data to confirm changes
str(new_df) # because in first steps, convert all columns to numeric just check them

#Check NA values
# Sum of NA values in each column
na_count <- sapply(new_df, function(x) sum(is.na(x)))
# Print the results
print(na_count)
# Apply linear interpolation to each column of the dataset
# We use lapply to apply the function to each column separately
data_interpolated <- data.frame(lapply(new_df, function(x) {
  # Check if there are any NA values in the column
  if (any(is.na(x))) {
    # Perform linear interpolation
    return(na.approx(x, na.rm = FALSE))
  } else {
    # Return the column unchanged if no NA values are present
    return(x)
  }
}))
na_count <- sapply(data_interpolated, function(x) sum(is.na(x)))
# Print the results to recheck NA 
print(na_count)



#Delete variables with limited variation
#Checking variables with limited variation
summary(data_interpolated)
#Removing variables with limited variation
data_interpolated <- data_interpolated[, !(names(data_interpolated) %in% c("RAINC", "SNOW", "SMOIS", "TSLB"))]
#Check the structure of the data after removal
summary(data_interpolated)
```

# Exploratory Data Analysis (EDA)

In this part, exploratory data analysis seeks out structures, ouliers, or distribution in wind speed data that may have an impact on prediction models. This stage is necessary to lay the framework for robust, data-driven insights.
```{r EDA}
# Basic statistics
summary(data_interpolated$WindSpeed)


#Outlier Detection
#Find outliers with Boxplot
boxplot(data_interpolated$WindSpeed, 
        main = "Boxplot of Wind Speed at Dogger Bank",
        ylab = "Wind Speed (m/s)",
        col = "lightblue",
        border = "blue")

#Check outliers
#Before check outliers is needed to have just numeric columns
numeric_columns <- sapply(data_interpolated, is.numeric) 
data_numeric <- data_interpolated[, numeric_columns]
all(sapply(data_numeric, is.numeric))

# Function to find outliers using IQR
iqr_outliers <- function(x) {
  Q1 <- quantile(x, 0.25)
  Q3 <- quantile(x, 0.75)
  IQR <- Q3 - Q1
  lower <- Q1 - 1.5 * IQR
  upper <- Q3 + 1.5 * IQR
  return(which(x < lower | x > upper))
}
# Apply the function to each numeric column
iqr_outliers_list <- sapply(data_numeric, iqr_outliers)
iqr_outliers_list  # Display indices of outliers
summary(data_numeric)
#The outliers that are detected, all they are in the range of variables. So it doesn't need to handle.


# Normality Check
# QQplot of normally distributed values
qqnorm(data_interpolated$WindSpeed, main = "Normal Q-Q Plot of wind speed")
# Add qqline to plot
qqline(data_interpolated$WindSpeed, col = "blue")

```

These figures illustrate that, while wind speeds on Dogger Bank are usually modest and consistent, they can be unusually high or low at times. Identifying high wind speed outliers can help with planning, risk management, and wind energy optimisation. This combined understanding of wind speed extremes and averages enables Dogger Bank to build more exact models for wind resource forecasts and management.

# Statistical Analyis

The study's following section includes a statistical analysis to validate the hypotheses based on the original research questions. This section examines the correlations and impacts of several environmental factors on wind speed at Dogger Bank using univariate, bivariate, and multivariate analysis.For correlation test, according Shapiro-Wilk Test result that shows non-parametric and the type of data that is numeric except date_time column, spearman is selected.

## Univariate Analysis
#####  Examining Wind Speed Distribution
```{r uni-analysis}

# Histogram of Wind Speed
hist(new_df$WindSpeed, main = "Histogram of Wind Speed", xlab = "Wind Speed (m/s)", col = "blue", breaks = 31)



# Perform Shapiro-Wilk Test for wind speed
shapiro_test <- shapiro.test(new_df$WindSpeed)
# show result
shapiro_test
#p<0.05 reject null hypothesis,data is not normally distributed.
```

The wind speed data's Shapiro-Wilk test result clearly indicates that it does not follow a normal distribution. This supports the use of non-parametric techniques for further statistical analysis by validating the presence of skewness or other non-normal features found in the histogram.

## Bivariate Analysis
##### Analyse the relationship between specific humidity (Q2) and wind speed at Dogger Bank
```{r bi-analysis1}

# Plotting the relationship between Specific Humidity and Wind Speed
ggplot(data_interpolated, aes(x = Q2, y = WindSpeed)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", color = "blue", se = FALSE) +
  labs(title = "Relationship between Specific Humidity and Wind Speed",
       x = "Specific Humidity (kg/kg)", y = "Wind Speed (m/s)")
```

The scatter plot indicates a small inverse relationship between specific humidity (Q2) and wind speed at Dogger Bank, as evidenced by the trend line's negative slope.

```{r bi-analysis2}
#Correlation Test
cor.test(data_interpolated$WindSpeed, data_interpolated$Q2, method = "spearman")
#p<0.05 reject null hypothesis, that means correlation between Q2 and Wind Speed

```

The value rho indicates a slightly negative association (-0.1854578) between wind speed and Q2.The p-value of 0.003375 suggests that this association is statistically significant, implying that it was unlikely to happen by chance if there was no true relationship.

```{r bi-analysis3}

linear_model <- lm(WindSpeed ~ Q2, data = data_interpolated)

summary(linear_model)  # Displays the regression output including coefficients
```

The analysis shows a significant intercept for the base wind speed at 13.088 m/s when specific humidity is zero, and a substantial negative slope of -955.185 for specific humidity, indicating that an increase in humidity significantly decreases wind speed by about 955 m/s. Although the model explains about 11.72% of the variability in wind speed, demonstrating that other factors also significantly influence wind speed, the highly significant F-statistic confirms the relationship between specific humidity and wind speed is statistically reliable and not random.


According to the findings, the correlation between specific humidity (Q2) and wind speed is statistically significant (p < 0.05), indicating that it is not due to random chance. However, the model explains only 11.72% of the variability in wind speed, suggesting that the impact of specific humidity, while significant, is limited.

## Multivariate Analysis

The purpose of multivariate analysis in this study is to understand how different climatic variables affect wind speed at Dogger Bank at the same time.

#### Checking Multiple Regression Assumptions
```{r multi-analysis1}
#Assumptions
#1. Have linear relationship (scatter plot)
#2. Errors/Residuals are normally distributed
#3. Errors are independent and there is no autocorrelation between errors
#4. Constant error variance- homoscedasticity of residuals or equal variance 
#i.e. variance around regression line is same for all values of other predictor variables
#5. No multi-collinearity between predictors
head(data_interpolated)
dataset <- data_interpolated[2:6]

regressor<- lm(WindSpeed ~., dataset) 
#summary(regressor)


#1. Have linear relationship (scatter plot)
pairs(dataset)
```

In the scatterplot matrix, WindSpeed does not show any clear relationships with TSK, PSFC, or Q2, indicating no strong or linear correlations between these variables and wind speed. Similarly, with RAINNC, there is no obvious pattern as the points are broadly scattered, suggesting no direct relationship exists.

```{r multi-analysis11}
#Check for linearity
#rainbow test for checking linearity (lmtest package)
#p<0.05 means non-linearity
raintest(regressor)
#The result show that data is non-linearity.

#2. Errors/Residuals are normally distributed
ols_plot_resid_hist(regressor)
ols_plot_resid_qq(regressor)
shapiro.test(regressor$residuals) #p>0.05 so distribution is normal
#Result shows that is non-distribution.

#3. Errors are independent and there is no autocorrelation between errors
#null hypo: there is no autocorrelation (errors are independent)
dwtest(regressor) #since p>0.05 - there is no autocorrelation

# The result shows autocorrelation and reject null hypothesis.

#4. Constant error variance- homoscedasticity of residuals or equal variance
#No hetroscedasticity
#homoscedasticity:variance around regression line is same for all values of the predictor variables
ncvTest(regressor) #p>0.05, no hetroscedasticity
#The result show that is hetroscedasticity because it's less than 0.05.

# 5. No multi-collinearity between predictors
#No multicollinearity
# two or more predictor variables are highly correlated
vif(regressor)  #vif>10 strong multicollinearity
#Results shows that data is multicollinearity because vif is less than 10

#summary(regressor)$r.squared

#lm_model <- lm(WindSpeed ~ TSK + PSFC + Q2 + RAINNC, data = data_interpolated)
summary(regressor)

```

The regression analysis of wind speed data at Dogger Bank reveals substantial limitations due to key assumptions being violated: the presence of non-linearity confirmed by the Rainbow test (p-value = 0.002937), heteroscedasticity indicated by a Non-constant Variance Score Test (p-value = 4.4062e-05), and non-normally distributed residuals as shown by the Shapiro-Wilk test (p-value = 0.004895). Additionally, significant autocorrelation in the residuals (p-value < 2.2e-16), despite acceptable VIF values, complicates the modelâ€™s reliability. With an adjusted R-squared value of 0.2253, indicating limited explanatory power, the results suggest that multiple linear regression is unsuitable for accurately predicting wind speeds at Dogger Bank. 

#### Correlation Analysis & Heatmap 
```{r multi-analysis2}


cor_matrix <- cor(data_interpolated[, c("WindSpeed", "RAINNC", "PSFC", "Q2", "TSK")], method = "spearman")
#Correlation Analysis 
#cor_matrix <- cor(data_interpolated[, sapply(data_interpolated, is.numeric)], method = "spearman")
print(cor_matrix)


#Heatmap
cor_data <- melt(cor_matrix)
ggplot(cor_data, aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1, 1)) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "", y = "", title = "Heatmap of Correlation Matrix", fill = "Correlation")

```

Q2 and WindSpeed (-0.1855): This negative correlation suggests that higher levels of specific humidity are slightly associated with lower wind speeds, which might indicate moisture-laden air being heavier and potentially less mobile.

RAINNC and WindSpeed (0.1625): A moderate positive correlation, which might imply that increased cumulative rainfall (RAINNC) correlates with slight increases in wind speed, perhaps due to the atmospheric disturbances rain can cause.

PSFC and WindSpeed (0.0287): Shows a very weak positive correlation between surface pressure and wind speed, suggesting that changes in surface pressure have a minimal direct impact on wind speed.

# Time Series
In this part, we analyse wind speed data to improve forecasting accuracy by using machine learning methods integrated with time-series analysis. We explore linear modeling and SARIMA models. 

#### STEP1: Read dataset
```{r read-dataset}
dataset <- data_interpolated
dataset$date_time <- as.POSIXct(dataset$date_time, format = "%d-%m-%Y-%H-%M")
timeseries_data <- dataset[, c(1, ncol(dataset))]
dim(timeseries_data)
```

#### STEP2: Convert to TS
```{r convert-ts}
ts_data <- ts(timeseries_data$WindSpeed, frequency = 8)
head(ts_data)
```

#### STEP3: Plotting the time series data
```{r plot-initial}
#Plotting the initial time series data
plot(ts_data, main="Initial Time Series Data", xlab = "Time", ylab = "Wind Speed")
```

This plot displays the initial time series of wind speed at Dogger Bank, illustrating the variable nature of wind speed over time.

##### Decompose the time series data

The data is decomposed into observed, seasonality, trend, and randomness to understand its patterns.
```{r decompose}
decomposed_data <- decompose(ts_data)
plot(decomposed_data)
```

The decomposition of the time series clearly shows seasonality, indicated by the regular, continuous pattern in the seasonal component. The trend component reveals gradual changes over time, that it's not completely increasing or decreasing.

#### STEP4: Handling missing values and outliers
```{r handle-missing}
#check missing value
missing_values_count <- sum(is.na(ts_data))
cat("Number of missing values:", missing_values_count, "\n")

#check outliers
outliers <- tsoutliers(ts_data)
outliers

#ts_data_clean <- tsclean(ts_data)
#plot(ts_data_clean, main = "Cleaned Wind Speed Time Series Data", xlab = "Time", ylab = "Wind Speed")
```

The results show 0 missing value and outliers, so it doesn't need any tsclean function.

#### STEP5: Check for stationarity

To ensure reliable forecasting, data should be check for stationary.
```{r stationarity}
#Perform the Augmented Dickey-Fuller Test.
adf_test_result <- adf.test(ts_data, alternative="stationary")
print(adf_test_result)

#Handle non-stationary, by perform diff function
ts_data_diff <- diff(ts_data)
adf_test_diff <- adf.test(ts_data_diff, alternative="stationary")
print(adf_test_diff)
```

The initial result of the ADF test resulted a p-value of 0.06339, which, being higher than 0.05, hence that the data is not stationary, and the null hypothesis of non-stationarity is accepted. After applying the differencing function and retesting, the p-value dropped to 0.01, confirming that our data has become stationary.


### STEP6: Splitting data into training and test sets
```{r splitting}
# Split into training and testing sets
train_data <- window(ts_data_diff, end=c(21,7))
test_data <- window(ts_data_diff, start=c(21,8)) 

# Summary of Splits
cat("Training Data Size:", length(train_data), "\n")
cat("Testing Data Size:", length(test_data), "\n")
```

### STEP7: Model creation with tslm (Time Series Linear Model)
```{r ARIMA}
#Manual ARIMA
#ACF and PACF plots
#Auto-correlation
acf(ts_data, main="ACF for Wind Speed",lag.max = 20) 
#Partial correlation
pacf(ts_data, main="PACF for Wind Speed", lag.max = 20)

#Test AR model
model_arima1<- Arima(train_data, order = c(1,0,0))
model_arima1
checkresiduals(model_arima1)
#Test MA model
model_arima2<- Arima(train_data, order = c(0,0,1))
model_arima2
checkresiduals(model_arima2)
#Test combination models of AR and MA.
model_arima3<- Arima(train_data, order = c(1,0,1))
model_arima3
checkresiduals(model_arima3)
```

According the autocorrelation results in checkresiduals and AIC value(510.15) the best model of manual ARIMA is c(0,0,1).

```{r tslm}
#check linearity
model<- lm(WindSpeed ~., timeseries_data)
raintest(model) 
#Because p value is upper that 0.05 we can use linear model (p-value = 0.05434).

# Build a linear model considering seasonality and trend
model_tslm <- tslm(train_data ~ season + trend)
model_forecast_tslm <- forecast(model_tslm, h = 80)

# Plot Forecast
plot(model_forecast_tslm, main="Forecast with Linear Regression", xlab="Time", ylab="Value")
autoplot(model_forecast_tslm) + autolayer(ts_data_diff) + ggtitle("TSLM Hourly Forecast")

```

### SARIMA modeling
```{r sarima-modeling}
# Build and SARIMA models
# According decompose seasonal diagram showed that time-series data is seasonal, we can use sarima model.
#fit_sarima <- auto.arima(train_data, seasonal=TRUE)
fit_sarima <- auto.arima(train_data,
                              stepwise=FALSE,
                              approximation=FALSE,
                              seasonal=TRUE, # This will extent to SARIMA
                              allowdrift=FALSE,
                              # parallel = TRUE,  # speeds up computation, but tracing not available
                              trace=TRUE)

# Forecast future points
forecast_sarima <- forecast(fit_sarima, h = 80)

# Plot Forecasts
autoplot(forecast_sarima) + autolayer(ts_data_diff) + ggtitle("SARIMA Hourly Forecast")
```

The optimal SARIMA model, ARIMA(2,0,1)(2,0,0)[8], with zero mean, has the lowest AIC value of 503.985.

### STEP8: Check accuracy
```{r accuracy}

accuracy_sarima <- accuracy(forecast_sarima, test_data)
accuracy_tslm <-accuracy(model_forecast_tslm,test_data)

cat("SARIMA Accuracy:\n")
print(accuracy_sarima)

cat("TSLM Accuracy:\n")
print(accuracy_tslm)

```
#### Evaluation of Accuracy Test for SARIMA and TSLM Models

Both models perform well in managing new data, as indicated by lower RMSE and MAE on test sets. However, both models are affected by percentage errors, which could be due to anomalies or specific characteristics of the wind speed data. The ACF1 values for both models show small residual autocorrelations, which might be addressed with model adjustments. The SARIMA model seems to handle the data slightly better, given the context of lower autocorrelation and slightly better percentage errors on the test data. 

### STEP9: Check residuals
```{r check-sarima}

checkresiduals(fit_sarima)

shapiro.test(fit_sarima$residuals)
```

#### Evaluation of Checking Residuals for SARIMA Model: 

The ARIMA(2,0,1)(2,0,0)[8] model used in this study indicates that the residuals fluctuate randomly about zero with no trends and closely resemble a normal distribution but the Shapiro test shows not normally distribution for residual because the p value is less than 0.05. The Ljung-Box test, with a p-value of 0.5469, indicates that these residuals do not show autocorrelation.Finally, the result shows that this model is well-suited for the dataset.

```{r check-tslm}

checkresiduals(model_tslm)

shapiro.test(model_tslm$residuals)
```

#### Evaluation of Checking Residuals for TSLM Model: 

Analysing  the linear regression model designed to anticipate wind speed, the presence of significant spikes in the residual plot shows outliers, which are most likely caused by storm occurrences, and the Shapiro test shows not normally distribution for residual because the p value is less than 0.05. Furthermore, the ACF plot demonstrates autocorrelation, which is confirmed by the Breusch-Godfrey test, which yields a p-value of 0.0001445, demonstrating significant serial correlation in residuals.This shows that the model is unsuitable for our dataset.However, the best result for this dataset was found using the last model, SARIMA.

# Machine Learning Prediction Algorithms

In our investigation into optimising wind speed predictions at Dogger Bank, we performed a comparative analysis of various machine learning regression techniques, including Decision Trees, Support Vector Regression (SVR), and multiple configurations of Random Forests, specifically models with 100, 150, and 200 trees. We evaluated each model's performance using MAE and RMSE metrics.

```{r compare-models}
# Load and prepare the data
head(data_interpolated)
dataset <- data_interpolated[2:6]

#Split data into test and train
set.seed(123)  
split=sample.split(dataset$WindSpeed, SplitRatio = 0.8)
training_set <- subset(dataset, split == TRUE)
testing_set <- subset(dataset, split== FALSE)
glimpse(training_set)

train_set_scaled <- as.data.frame(scale(training_set)) #return as dataframe
colnames(train_set_scaled) <- colnames(training_set)   #used the previous name of columns to new dataframe
test_set_scaled <- as.data.frame(scale(testing_set))
colnames(test_set_scaled) <- colnames(testing_set)

#Random Forest with varying number of trees
model_rf100 <- randomForest(WindSpeed ~ ., data = training_set, ntree = 100)
predictions_rf100 <- predict(model_rf100, testing_set)
mae_rf100 <- MAE(predictions_rf100, testing_set$WindSpeed)
rmse_rf100 <- RMSE(predictions_rf100, testing_set$WindSpeed)

model_rf150 <- randomForest(WindSpeed ~ ., data = training_set, ntree = 150)
predictions_rf150 <- predict(model_rf150, testing_set)
mae_rf150 <- MAE(predictions_rf150, testing_set$WindSpeed)
rmse_rf150 <- RMSE(predictions_rf150, testing_set$WindSpeed)

model_rf200 <- randomForest(WindSpeed ~ ., data = training_set, ntree = 200)
predictions_rf200 <- predict(model_rf200, testing_set)
mae_rf200 <- MAE(predictions_rf200, testing_set$WindSpeed)
rmse_rf200 <- RMSE(predictions_rf200, testing_set$WindSpeed)

# Fit a Decision Tree model
model_dt <- rpart(WindSpeed ~ ., data = training_set, control = rpart.control(minsplit = 1))
predictions_dt <- predict(model_dt, testing_set)
mae_dt <- MAE(predictions_dt, testing_set$WindSpeed)
rmse_dt <- RMSE(predictions_dt, testing_set$WindSpeed)

# Fit a Support Vector Regression model
model_svr <- svm(WindSpeed ~ ., data = train_set_scaled, type <- 'eps-regression', kernel <- 'radial') #non-linear regression
predictions_svr <- predict(model_svr, test_set_scaled)
mae_svr <- MAE(predictions_svr, test_set_scaled$WindSpeed)
rmse_svr <- RMSE(predictions_svr, test_set_scaled$WindSpeed)

predict(model_rf100, data.frame(TSK=280, PSFC=100053 ,Q2=0.005, RAINNC=0.1))
predict(model_rf150, data.frame(TSK=280, PSFC=100053 ,Q2=0.005, RAINNC=0.1))
predict(model_rf200, data.frame(TSK=280, PSFC=100053 ,Q2=0.005, RAINNC=0.1))
predict(model_dt, data.frame(TSK=280, PSFC=100053 ,Q2=0.005, RAINNC=0.1))
predict(model_svr, data.frame(TSK=280, PSFC=100053 ,Q2=0.005, RAINNC=0.1))

# Store results in a data frame
results <- data.frame(Model = c("Random Forest_100","Random Forest_150","Random Forest_200","SVR", "Decision Tree"),
                      MAE = c(mae_rf100,mae_rf150,mae_rf200,mae_svr, mae_dt),
                      RMSE = c(rmse_rf100,rmse_rf150,rmse_rf200,rmse_svr, rmse_dt))

# Melt data for visualization
results_long <- melt(results, id.vars = "Model", variable.name = "Metric", value.name = "Value")

# Plot the results
ggplot(results_long, aes(x = Model, y = Value, fill = Metric)) +
  geom_bar(stat = "identity", position = position_dodge(), width = 0.6) +
  geom_text(aes(label = round(Value, 2)), position = position_dodge(width = 0.6), vjust = -0.25, size = 3.5) +
  scale_y_continuous(labels = scales::comma) +  # Formats the y-axis values to include commas for thousands
  labs(title = "Comparison of Regression Models", x = "Model", y = "Metric Value") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),  # Adjusts the angle and horizontal adjustment of x-axis labels
        axis.text.y = element_text(size = 12))  # Increases the size of y-axis text for better readability

```

This bar chart compares the Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) of various regression models. Notably, the Decision Tree model has the greatest error rates, while the Support Vector Regression (SVR) has the lowest, showing that it is the most accurate at forecasting wind speed. For Random Forest models, the error decreases as the number of trees increases up to 150; however, the model with 200 trees does not follow this trend and shows a slight increase in error compared to the 150-tree model, meaning that adding more trees does not consistently reduce prediction error and that an optimal number of trees may exist.